Wraz ze wzrostem stopnia wielomianu maleje błąd dopasowania dla danych treningowych. Wynika to z tego, że na ogół zależności między zmiennymi nie są liniowe, więc modele o wyższych stopniach są w stanie lepiej wyrazić skomplikowane zależności. Jednak dla dużej liczby parametrów model zaczyna się uczyć danych 'na pamięć', (zjawisko overfittingu), przez co dopasowuje się idealnie do danych uczących, ale predykcje dla nowych danych są gorsze niż dla modeli o mniejszej liczbie parametrów. Jako że zdolność do predykcji świadczy o przydatności modelu, najlepiej będzie wybrać taki, który dobrze się generalizuje na nowe przypadki (posiada odpowiednio mały błąd dla danych testowych).
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os \n",
    "from functools import partial\n",
    "import scipy\n",
    "from scipy.optimize import fmin\n",
    "DATA_DIR = r\"C:\\Users\\julia\\VSCode\\MSiD\\data\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1\n",
    "W metodzie stałokrokowej nie mamy gwarancji osiągnięcia minimum, gdyż przy stałym kroku możemy nawet przeskoczyć minimum lokalne "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent(func, gradient, learning_rate, start_point, stop_condition, write_to=None):\n",
    "    current_point = start_point\n",
    "    i = 1\n",
    "    while True:\n",
    "        print(f\"ITERATION {i}\", file=write_to)\n",
    "        gradient_value = gradient(current_point)\n",
    "        print(f\"\\tgradient_value = {gradient_value}\", file=write_to)\n",
    "        step_size = learning_rate * gradient_value\n",
    "        print(f\"\\tstep_size = {step_size}\", file=write_to)\n",
    "        next_point = current_point - step_size \n",
    "        print(f\"\\tnext_point = {next_point}\", file=write_to)\n",
    "        print(f\"\\tf({next_point}) = {func(next_point)}\", file=write_to)\n",
    "\n",
    "        if stop_condition(current_point, next_point, write_to=write_to):\n",
    "            print(f\"Reached stop condition at {next_point}, value = {func(next_point)}\", file=write_to)\n",
    "            break \n",
    "        i += 1\n",
    "        current_point = next_point\n",
    "    \n",
    "\n",
    "    return current_point\n",
    "\n",
    "\n",
    "def func(X):\n",
    "    return np.sum(X ** 2)\n",
    "\n",
    "def gradient(X):\n",
    "    return X * 2\n",
    "\n",
    "def L1_norm(X):\n",
    "    return np.sum(np.abs(X))\n",
    "\n",
    "def stop(current_point, next_point, error, write_to=None):\n",
    "    computed_L1 = L1_norm(current_point - next_point)\n",
    "    print(f\"L1 = {computed_L1}\", file=write_to) \n",
    "    return computed_L1 < error \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f([0.0065536 0.0065536]) = 8.589934592000008e-05\n"
     ]
    }
   ],
   "source": [
    "h_i = 0.3\n",
    "x_start = np.array([4, 4])\n",
    "with open(os.path.join(DATA_DIR, \"ex1.txt\"), mode='w') as f:\n",
    "    result = steepest_descent(func, gradient, h_i, x_start, partial(stop, error=0.01), write_to=f)\n",
    "    print(f\"f({result}) = {func(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = np.array([3, 4])\n",
    "v2 = np.array([1, 1])\n",
    "\n",
    "# Subtraction is element-wise\n",
    "v1 - v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 44\n",
      "         Function evaluations: 80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.503461544006107e-10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy_min = fmin(func, x0=x_start)\n",
    "func(scipy_min)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2\n",
    "Minimum tej funkcji znajduje się w punkcie (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f([0.54009587 0.18933999]) = 0.237707553462972\n"
     ]
    }
   ],
   "source": [
    "def func(X):\n",
    "    return 2.5 * (X[0] ** 2 - X[1]) ** 2 + (1 - X[0]) ** 2\n",
    "\n",
    "def gradient(X):\n",
    "    return np.array([10 * X[0] ** 3 - 10 * X[0] * X[1] + 2 * X[0] - 2, \n",
    "                     -5 * (X[0] ** 2 - X[1])])\n",
    "\n",
    "h_i = 0.1\n",
    "x_start = np.array([-0.5, 1])\n",
    "with open(os.path.join(DATA_DIR, \"ex2.txt\"), mode='w') as f:\n",
    "    result = steepest_descent(func, gradient, h_i, x_start, partial(stop, error=0.1), write_to=f)\n",
    "    print(f\"f({result}) = {func(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f([0.9219545  0.83663145]) = 0.0065379022322644325\n",
      "f([0.92197522 0.83668903]) = 0.00653337401708192\n"
     ]
    }
   ],
   "source": [
    "h_i = 0.1\n",
    "# Iteration 37\n",
    "with open(os.path.join(DATA_DIR, \"ex2_a.txt\"), mode='w') as f:\n",
    "    result = steepest_descent(func, gradient, h_i, x_start, partial(stop, error=0.01), write_to=f)\n",
    "    print(f\"f({result}) = {func(result)}\")\n",
    "\n",
    "h_i = 0.01\n",
    "with open(os.path.join(DATA_DIR, \"ex2_b.txt\"), mode='w') as f:\n",
    "    result = steepest_descent(func, gradient, h_i, x_start, partial(stop, error=0.001), write_to=f)\n",
    "    print(f\"f({result}) = {func(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func(np.array([1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 58\n",
      "         Function evaluations: 109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.3379829250198353e-10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy_min = fmin(func, x0=x_start)\n",
    "func(scipy_min)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W porównaniu z metodą zmiennokrokową, w metodzie stałokrokowej nie wykonujemy dodatkowych obliczeń, więc możemy więcej iteracji zrobić"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_steepest_descent(func, gradient, learning_rate, start_point, stop_condition, write_to=None):\n",
    "    current_point = start_point\n",
    "    i = 1\n",
    "    while True:\n",
    "        print(f\"ITERATION {i}\", file=write_to)\n",
    "        gradient_value = gradient(current_point)\n",
    "        print(f\"\\tgradient_value = {gradient_value}\", file=write_to)\n",
    "        step_size = learning_rate(current_point, gradient_value, write_to) * gradient_value\n",
    "        print(f\"\\tstep_size = {step_size}\", file=write_to)\n",
    "        next_point = current_point - step_size \n",
    "        print(f\"\\tnext_point = {next_point}\", file=write_to)\n",
    "        print(f\"\\tf({next_point}) = {func(next_point)}\", file=write_to)\n",
    "\n",
    "        if stop_condition(current_point, next_point, write_to=write_to):\n",
    "            print(f\"Reached stop condition at {next_point}, value = {func(next_point):.2f}\", file=write_to)\n",
    "            break \n",
    "        i += 1\n",
    "        current_point = next_point\n",
    "    \n",
    "\n",
    "    return current_point\n",
    "\n",
    "\n",
    "def update_learning_rate(X, gradient, write_to=None):\n",
    "    \n",
    "    def metric(alpha):\n",
    "        # This is original function\n",
    "        return np.sum((X - alpha * gradient) ** 2)\n",
    "    \n",
    "    min_alpha = fmin(metric, x0=0.01, disp=False)\n",
    "    print(f\"\\talpha = {min_alpha[0]:.2f}\", file=write_to)\n",
    "    return min_alpha[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W metodzie zmiennokrokowej wykonujemy optymalizację kierunkową współczynnika kroku. Tutaj wybieramy takie $\\alpha$, by minimalizowało funkcję wejściową"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\talpha = 0.50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5000000000000007"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_learning_rate(np.array([4, 4]), np.array([8, 8]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutaj potrzebowaliśmy jednego kroku, by znaleźć minimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f([-5.32907052e-15 -5.32907052e-15]) = 5.679798517591285e-29\n"
     ]
    }
   ],
   "source": [
    "def func(X):\n",
    "    return np.sum(X ** 2)\n",
    "\n",
    "def gradient(X):\n",
    "    return X * 2\n",
    "\n",
    "x_start = np.array([4, 4])\n",
    "with open(os.path.join(DATA_DIR, \"ex3.txt\"), mode='w') as f:\n",
    "    result = var_steepest_descent(func, gradient, update_learning_rate, x_start, partial(stop, error=0.01), write_to=f)\n",
    "    print(f\"f({result}) = {func(result)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_steepest_descent_2(func, gradient, learning_rate, start_point, stop_condition, write_to=None):\n",
    "    current_point = start_point\n",
    "    i = 1\n",
    "    while True:\n",
    "        print(f\"ITERATION {i}\", file=write_to)\n",
    "        gradient_value = gradient(current_point)\n",
    "        print(f\"\\tgradient_value = {gradient_value}\", file=write_to)\n",
    "        step_size = learning_rate(i, write_to) * gradient_value\n",
    "        print(f\"\\tstep_size = {step_size}\", file=write_to)\n",
    "        next_point = current_point - step_size \n",
    "        print(f\"\\tnext_point = {next_point}\", file=write_to)\n",
    "        print(f\"\\tf({next_point}) = {func(next_point)}\", file=write_to)\n",
    "\n",
    "        if stop_condition(current_point, next_point, write_to=write_to):\n",
    "            print(f\"Reached stop condition at {next_point}, value = {func(next_point):.2f}\", file=write_to)\n",
    "            break \n",
    "        i += 1\n",
    "        current_point = next_point\n",
    "    \n",
    "\n",
    "    return current_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr1(i, write_to=None):\n",
    "    next_step = 0.0007 * i \n",
    "    print(f\"\\tnext step = {next_step:.2f}\", file=write_to)\n",
    "    return next_step\n",
    "\n",
    "def lr2(i, write_to=None):\n",
    "    next_step = 0.000701 * i \n",
    "    print(f\"\\tnext step = {next_step:.2f}\", file=write_to)\n",
    "    return next_step\n",
    "\n",
    "def lr3(i, write_to=None):\n",
    "    next_step = 0.03 * np.sqrt(i) \n",
    "    print(f\"\\tnext step = {next_step:.2f}\", file=write_to)\n",
    "    return next_step\n",
    "\n",
    "def lr4(i, write_to=None):\n",
    "    next_step = 0.03* np.sqrt(np.sqrt(i))\n",
    "    print(f\"\\tnext step = {next_step:.2f}\", file=write_to)\n",
    "    return next_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[a]: f([0.9931332  0.98519829]) = 5.026245143622128e-05\n",
      "[b]: f([0.9931852  0.98527724]) = 4.9688200256330327e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\AppData\\Local\\Temp\\ipykernel_8604\\246558139.py:5: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return 2.5 * (X[0] ** 2 - X[1]) ** 2 + (1 - X[0]) ** 2\n",
      "C:\\Users\\julia\\AppData\\Local\\Temp\\ipykernel_8604\\246558139.py:8: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return np.array([10 * X[0] ** 3 - 10 * X[0] * X[1] + 2 * X[0] - 2,\n",
      "C:\\Users\\julia\\AppData\\Local\\Temp\\ipykernel_8604\\246558139.py:9: RuntimeWarning: overflow encountered in double_scalars\n",
      "  -5 * (X[0] ** 2 - X[1])])\n",
      "C:\\Users\\julia\\AppData\\Local\\Temp\\ipykernel_8604\\246558139.py:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2.5 * (X[0] ** 2 - X[1]) ** 2 + (1 - X[0]) ** 2\n",
      "C:\\Users\\julia\\AppData\\Local\\Temp\\ipykernel_8604\\246558139.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.array([10 * X[0] ** 3 - 10 * X[0] * X[1] + 2 * X[0] - 2,\n",
      "C:\\Users\\julia\\AppData\\Local\\Temp\\ipykernel_8604\\246558139.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  -5 * (X[0] ** 2 - X[1])])\n"
     ]
    }
   ],
   "source": [
    "methods = [lr1, lr2, lr3, lr4]\n",
    "names = ['a', 'b', 'c', 'd']\n",
    "\n",
    "def func(X):\n",
    "    return 2.5 * (X[0] ** 2 - X[1]) ** 2 + (1 - X[0]) ** 2\n",
    "\n",
    "def gradient(X):\n",
    "    return np.array([10 * X[0] ** 3 - 10 * X[0] * X[1] + 2 * X[0] - 2, \n",
    "                     -5 * (X[0] ** 2 - X[1])])\n",
    "\n",
    "\n",
    "x_start = np.array([-0.5, 1])\n",
    "\n",
    "for method, name in zip(methods, names):\n",
    "    with open(os.path.join(DATA_DIR, f\"ex4{name}.txt\"), mode='w') as f:\n",
    "        result = var_steepest_descent_2(func, gradient, method, x_start, partial(stop, error=0.001), write_to=f)\n",
    "        print(f\"[{name}]: f({result}) = {func(result)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataCampTutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
